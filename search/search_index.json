{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to The Data Protection Codex This site is dedicated to the documentation of best practices for modern data protection. With the value of data increasing in importance in the current landscape, the timely protection of data has never had more significance. This online book is divided into multiple sections and chapters. Contents Chapter 1 - A Short History of Data Chapter 2 - Why Protect Data? Chapter 3 - Defining Data Protection Chapter 4 - ETIL Chapter 5 - A Short History of Data Protection Chapter 6 - Diving Deeper into Data Protection","title":"Home"},{"location":"#welcome-to-the-data-protection-codex","text":"This site is dedicated to the documentation of best practices for modern data protection. With the value of data increasing in importance in the current landscape, the timely protection of data has never had more significance. This online book is divided into multiple sections and chapters.","title":"Welcome to The Data Protection Codex"},{"location":"#contents","text":"Chapter 1 - A Short History of Data Chapter 2 - Why Protect Data? Chapter 3 - Defining Data Protection Chapter 4 - ETIL Chapter 5 - A Short History of Data Protection Chapter 6 - Diving Deeper into Data Protection","title":"Contents"},{"location":"chapter1/","text":"A Short History of Data The expression \u201cdata is the lifeblood of business\u201d is a well-worn phrase that has garnered much use in recent years. It\u2019s certainly true that all businesses create and process data on a daily basis as part of normal operations, but the volume and diversity of information flowing through every type of organisation is greater than it ever was. Artificial Intelligence, as one current example, is driving demand for quantities of data where incremental gains in the accuracy of AI models is only seen with an order of magnitude of additional information. Businesses continually need more data to drive more insights and potential customer value. In reality, data has always been at the centre of business. Before the widespread use of technology, businesses would record transactions in physical ledgers, that would be written by hand and rarely reviewed. As digital technology was introduced from the 1950s, all types of businesses and organisations became increasingly dependent on the storage and retrieval of electronic records, as this digital data offered fast and automated analysis and insights compared to human review. One very early example of a company spotting the potential of computing is J Lyons & Co and the LEO computer , which dates back to 1951 and is widely regarded as the first computer created for commercial business applications. The first electronic computers developed in the 1940s were more arithmetic calculators than data processors. ENIAC was programmed manually to perform its operational tasks, with earlier machines performing similar tasks of complex computations. The \u201cBaby\u201d developed by the University of Manchester was the first stored program computer, which eventually formed the Harvard architecture and the Von Neumann architecture of all modern computing systems (with the arguable exception of quantum computing). The Von Neumann design consists of processing and memory functions, with data and code read from external input devices. This is what today we would consider persistent storage. Early computing platforms used magnetic tape for persistent storage ( UNIVAC being the first in 1951), followed by magnetic disk introduced by IBM in 1956 (as part of the IBM 305 system). Today we have a wide range of persistent storage media that still includes tape and disk but is increasingly moving towards solid-state technology such as NAND flash. There are also new recording media including resistive RAM and ceramic technologies that aim to directly compete with the dominant flash technology. Today, businesses of all types are almost 100% reliant on information technology to operate. Three decades ago, if computer systems went down, most businesses could revert to manual operations and catch up when computers systems came back online. Today, data is created from so many diverse sources (a topic we will discuss in a moment) that can\u2019t be generated manually. As a result, when computer systems are down, businesses aren\u2019t operating. Computer systems and the data they hold have become essential for the operation of a modern business. The Data Centre The terms \u201cdata processing\u201d and \u201cdata centre\u201d, now part of our accepted lexicon, were coined in the 1960s[CE1] and chosen with good reason. Computing devices at that time were big and expensive, requiring careful management and specific technical skills. Distributed networking barely existed (DARPANET, the forerunner of the modern Internet, went live in 1969[CE2]), so all business information was centralised on mainframe computing systems, essentially a single \u201ccentre\u201d for the processing of data. As networking has become ubiquitous and costs have plummeted, businesses have been able to provide remote computing capabilities, initially with \u201cdumb\u201d terminals, followed by distributed computing (now being rebranded as edge computing). Each of the last four decades has seen an expansion and diversification of computing infrastructure and solutions. The modern data centre is no longer a single place, but a virtual entity that is an aggregation of many disparate products, solutions[CE3] and operating models. Figure X shows the diversity of infrastructure and solutions used to deliver modern IT. Today, a typical business may have systems in an on-premises data centre, the public cloud, at the edge and use SaaS-based business tools. This mix can be fluid and the balance of data in each location may change over time. All of this data needs protecting in some way, because all of this information is critical to the operation of a modern enterprise[CE4]. Over the lifetime of a business, data is created, collected, and processed. In the early days of computing, most data were human-generated and stored in transactional databases. So called \u201cstructured data\u201d is now being overtaken by the creation of unstructured content, including information generated by both human and non-human sources (such as cameras, sensors or medical scanning equipment). All of this data has value to the business, either as part of day-to-day operations or for generating future value. Increasingly, unstructured data is being seen as a source for Artificial Intelligence, either to train large language models to to supplement LLM applications through the concept of retrieval augmented generation (RAG). During the last decade, data has become a strategic asset for businesses. Traditional data warehouses have become data \u201clakes\u201d that are mined using Artificial Intelligence software to generate inferences which can in turn be applied to new data entering the system. An AI model is \u201ctrained\u201d on huge quantities of data, then applies that knowledge to new data in order to discover insights or other information that couldn\u2019t easily be determined by humans. As AI models evolve, the volumes of data being processed now extend into multi-petabytes of capacity. Side Note: the volume of data needed for LLMs is now so vast that companies are creating \u201csynthetic\u201d data to further expand the accuracy and capability of LLM models. This is yet another source of information that must be retained for any LLM retraining process. Data Sources We mentioned earlier about the early forms of data creation as computers started to become widespread. When IT systems were first developed, data was entered manually from existing systems and processes. Data entry clerks transcribed paper records into electronic data for further processing[CE5]. With the widespread use of low-cost endpoint devices (starting with the personal computer), businesses were able to make data creation and data entry part of a typical job specification. Early desktop solutions included word processing, spreadsheets and rudimentary databases. Today, we would hardly consider adding computer skills on a job advertisement as the ability to use a keyboard and mouse are seen as ubiquitous and essential in any office-based job. From manually entered data typically stored in structured databases, the types and volume of business data has increased exponentially. Human-created data within a business, forms only a fraction of the data created within a commercial organisation every day. Much more information is created by customers using websites and applications, manufacturing and machinery creating products, data from sensors or other logging equipment, plus data created in the form of analytics or machine learning. There is now also a significant volume of data used to track activity across information technology (both good and bad), with many multiples of application data retained for archiving and data recovery. Data protection and poor data management practices can result in data sprawl, with many multiples of data duplication, with some estimates as high as 10x levels of redundant copies. By 2025, IDC estimates that approximately 175 zettabytes of data will be created annually, with a compound annual growth rate (CAGR) of around[CE6] 23%. However, only around 2% of data from 2020 was saved into 2021, demonstrating the transient nature of what we create. Of course, some data is filtered and consolidated, rather than being immediately discarded, a process which can\u2019t easily be recreated if that information is lost. Despite the attrition rate, there is constant pressure on IT organisations to store and manage ever greater volumes of data, with an increasingly unpredictable view on whether that information will ever be used again. Many businesses follow the mantra of \u201cretain everything\u201d, which has limits of sustainability and cost. Keeping all data forever also introduces risk for the business, if processes aren\u2019t sufficient to track and inventory content in a timely fashion. Data Lifecycle What is driving this instability on future data value? For many decades, data had a predictable value over its lifetime. New records in a database, for example, would be frequently accessed when the data was created, then see a decrease in activity over time, before being archived and eventually deleted. This curve was well known and predictable. Credit card processing is a good example, to illustrate data lifecycle. Historically, when a customer made a purchase, the transaction data would be frequently accessed by the both the customer\u2019s and merchant\u2019s bank. Assuming there were no queries with the transaction, the data would be archived after being used to produce a printed statement. Eventually the credit card company would delete the data, perhaps a fixed amount of time after an account was closed or based on whatever regulatory compliance rules required. The original transaction data would quickly lose value as it aged out. Today if a customer uses a credit card, the transaction might get reviewed through a mobile app or online. Banks generally enable customer searching of data a few years into the past. The bank and credit card provider may also use the data for real-time fraud detection, looking at past purchasing trends to determine whether to authorise or decline a transaction. Banks use historical customer data to determine credit limits, both upwards and downwards (many credit card providers will reduce a credit limit to mitigate losses if fraud occurs). Merchants will use credit card data to predict spending habits, for example, offering discount or emailing promotions to prospective buyers. One of the greatest challenges for businesses is to determine when and if data will be useful once past the point of initial creation. Where data used to decline in value over time, the increasing use of analytics means data can have some, as yet, unspecified future value, especially when combined with new data sources. In the early days of computing, businesses could predict that the volume of data within an organisation was generally proportional to the volume of business being done. Today, data is retained on the assumption of future value and an, as yet, unmet need to keep an organisation competitive. Changing Practices It is true to say that storage infrastructure has never been so cheap, when reviewing a simple $/GB benchmark. Barring one or two blips (due to manufacturing issues, for example the Thailand floods of 2011 ), storage costs have always declined rapidly and consistently over time. The cost of a new modern hard disk drive model, for example, has been static at around $650 when first introduced to the market, irrespective of capacity. This is because the bill of materials (BoM) hasn\u2019t changed much in two decades. Many businesses look at data storage and view the declining price of storage media as an opportunity to retain data forever. However, data management costs and data storage costs are not the same. In addition, the volume of data being stored in the enterprise is growing at a rate that is faster than the reduction in storage media costs when measured over the long term. Data management adds a premium on top of the cost of hardware acquisition. If data is retained on-premises, then there are also facilities costs such as data centre space, power and cooling. In the public cloud, data is charged at a monthly recurring fee for storage and transactional charges for access. The cost of data retention is therefore both a capital and operational cost, depending on where that data is located. Retaining data forever becomes a significant form of technical debt, if not managed correctly. Many organisations have no insight into what\u2019s being stored and where (back to our four application deployment models). If a business doesn\u2019t know what data assets it possesses, how can they adequately be protected? Finally, we should point out the challenges of regulation. In many industries (notably finance and healthcare), extensive regulation exists to ensure data is retained safely for many years past the point of its original use. Businesses can be fined heavily for breaching the rules that govern data management. Outside of industry-specific regulation, GDPR and other country-based constraints place responsibility on businesses to maintain responsible stewardship of personal data, including timely reporting of data loss or data breaches. In addition to rules on data management, governments are starting to introduce measures to ensure businesses can recover data in the event of a malware or ransomware attack. In 2025, the Digital Operational Resilience Act (DORA) became law in the EU, requiring business to improve their digital operational resilience. In 2023, the US government started development of the US Cyber Trust Mark , a labelling programme to ensure IoT devices confirm to a strong cybersecurity framework. Power and Responsibility The value of data provides businesses with competitive advantage, and used correctly has great power. However, at the same time, businesses have a responsibility to use data ethically and within the boundaries put in place by industry regulators. For both strategic and governance reasons, data must be protected from loss or corruption. We will touch on the regulatory issues of data management later in this book, but for now, we need to consider that data protection is an essential business function for all organisations. In the EU, the government introduced the EU AI Act , aimed at creating standards of governance for the use of data within AI-focused applications. The USA also introduced similar AI governance rules, but these were repealed by incoming President Donald Trump during his first days in office[CE7]. Backup versus Archiving We will come back and discuss the topic in more detail later in this book, but for now we should explain the difference between backup and archiving. Here are two straightforward definitions that explain the difference between the process of backup and archiving. Definition: Data protection is the creation of a point-in-time copy of data that enables recovery of that data at some point in the future, back to the previous time point. Definition: Archiving is the process of moving the primary copy of data out of storage systems to a secondary, (typically cheaper) location, with a view to retaining the data for future reference but not using that data in day-to-day business activities. As we can see, creating a backup results in an additional copy of data being taken, one that represents a static point in time we can return to at a later date. For various reasons, which we will discuss in a moment, it may be necessary to restore all or part of a backup to a previous point, due to issues with the main or current copy. An archive represents a different process, where data is moved out of a primary application and placed elsewhere for future reference, generally on cheaper storage media. Archiving has several benefits. Firstly, it saves on primary disk space that may be an expensive resource. Secondly, it makes applications work faster by reducing the volume of data in search and retrieval functions. The savings made from archiving aren\u2019t just in storage space but can result in lower memory requirements and reduced CPU utilisation. Third, archive data can be stored in a format that provides for better processing, for example, in AI applications or other OLAP applications. Historically, backup has been used as a \u201cpoor quality\u201d archive, with the assumption that data can be retrieved from a backup if needed. Data that would be otherwise archived is simply deleted from the primary system. This approach is risky, as the restore process can be protracted and lengthy. In addition, backups end up being retained much longer than is needed, simply to service the requirement for archived data recovery. In modern IT infrastructure, archived data can easily be moved into a data warehouse or data lake, with specialist tools to provide data insights and analysis. We\u2019ll touch on backup versus archiving in more detail later in this book.","title":"1. A Short History of Data"},{"location":"chapter1/#a-short-history-of-data","text":"The expression \u201cdata is the lifeblood of business\u201d is a well-worn phrase that has garnered much use in recent years. It\u2019s certainly true that all businesses create and process data on a daily basis as part of normal operations, but the volume and diversity of information flowing through every type of organisation is greater than it ever was. Artificial Intelligence, as one current example, is driving demand for quantities of data where incremental gains in the accuracy of AI models is only seen with an order of magnitude of additional information. Businesses continually need more data to drive more insights and potential customer value. In reality, data has always been at the centre of business. Before the widespread use of technology, businesses would record transactions in physical ledgers, that would be written by hand and rarely reviewed. As digital technology was introduced from the 1950s, all types of businesses and organisations became increasingly dependent on the storage and retrieval of electronic records, as this digital data offered fast and automated analysis and insights compared to human review. One very early example of a company spotting the potential of computing is J Lyons & Co and the LEO computer , which dates back to 1951 and is widely regarded as the first computer created for commercial business applications. The first electronic computers developed in the 1940s were more arithmetic calculators than data processors. ENIAC was programmed manually to perform its operational tasks, with earlier machines performing similar tasks of complex computations. The \u201cBaby\u201d developed by the University of Manchester was the first stored program computer, which eventually formed the Harvard architecture and the Von Neumann architecture of all modern computing systems (with the arguable exception of quantum computing). The Von Neumann design consists of processing and memory functions, with data and code read from external input devices. This is what today we would consider persistent storage. Early computing platforms used magnetic tape for persistent storage ( UNIVAC being the first in 1951), followed by magnetic disk introduced by IBM in 1956 (as part of the IBM 305 system). Today we have a wide range of persistent storage media that still includes tape and disk but is increasingly moving towards solid-state technology such as NAND flash. There are also new recording media including resistive RAM and ceramic technologies that aim to directly compete with the dominant flash technology. Today, businesses of all types are almost 100% reliant on information technology to operate. Three decades ago, if computer systems went down, most businesses could revert to manual operations and catch up when computers systems came back online. Today, data is created from so many diverse sources (a topic we will discuss in a moment) that can\u2019t be generated manually. As a result, when computer systems are down, businesses aren\u2019t operating. Computer systems and the data they hold have become essential for the operation of a modern business.","title":"A Short History of Data"},{"location":"chapter1/#the-data-centre","text":"The terms \u201cdata processing\u201d and \u201cdata centre\u201d, now part of our accepted lexicon, were coined in the 1960s[CE1] and chosen with good reason. Computing devices at that time were big and expensive, requiring careful management and specific technical skills. Distributed networking barely existed (DARPANET, the forerunner of the modern Internet, went live in 1969[CE2]), so all business information was centralised on mainframe computing systems, essentially a single \u201ccentre\u201d for the processing of data. As networking has become ubiquitous and costs have plummeted, businesses have been able to provide remote computing capabilities, initially with \u201cdumb\u201d terminals, followed by distributed computing (now being rebranded as edge computing). Each of the last four decades has seen an expansion and diversification of computing infrastructure and solutions. The modern data centre is no longer a single place, but a virtual entity that is an aggregation of many disparate products, solutions[CE3] and operating models. Figure X shows the diversity of infrastructure and solutions used to deliver modern IT. Today, a typical business may have systems in an on-premises data centre, the public cloud, at the edge and use SaaS-based business tools. This mix can be fluid and the balance of data in each location may change over time. All of this data needs protecting in some way, because all of this information is critical to the operation of a modern enterprise[CE4]. Over the lifetime of a business, data is created, collected, and processed. In the early days of computing, most data were human-generated and stored in transactional databases. So called \u201cstructured data\u201d is now being overtaken by the creation of unstructured content, including information generated by both human and non-human sources (such as cameras, sensors or medical scanning equipment). All of this data has value to the business, either as part of day-to-day operations or for generating future value. Increasingly, unstructured data is being seen as a source for Artificial Intelligence, either to train large language models to to supplement LLM applications through the concept of retrieval augmented generation (RAG). During the last decade, data has become a strategic asset for businesses. Traditional data warehouses have become data \u201clakes\u201d that are mined using Artificial Intelligence software to generate inferences which can in turn be applied to new data entering the system. An AI model is \u201ctrained\u201d on huge quantities of data, then applies that knowledge to new data in order to discover insights or other information that couldn\u2019t easily be determined by humans. As AI models evolve, the volumes of data being processed now extend into multi-petabytes of capacity. Side Note: the volume of data needed for LLMs is now so vast that companies are creating \u201csynthetic\u201d data to further expand the accuracy and capability of LLM models. This is yet another source of information that must be retained for any LLM retraining process.","title":"The Data Centre"},{"location":"chapter1/#data-sources","text":"We mentioned earlier about the early forms of data creation as computers started to become widespread. When IT systems were first developed, data was entered manually from existing systems and processes. Data entry clerks transcribed paper records into electronic data for further processing[CE5]. With the widespread use of low-cost endpoint devices (starting with the personal computer), businesses were able to make data creation and data entry part of a typical job specification. Early desktop solutions included word processing, spreadsheets and rudimentary databases. Today, we would hardly consider adding computer skills on a job advertisement as the ability to use a keyboard and mouse are seen as ubiquitous and essential in any office-based job. From manually entered data typically stored in structured databases, the types and volume of business data has increased exponentially. Human-created data within a business, forms only a fraction of the data created within a commercial organisation every day. Much more information is created by customers using websites and applications, manufacturing and machinery creating products, data from sensors or other logging equipment, plus data created in the form of analytics or machine learning. There is now also a significant volume of data used to track activity across information technology (both good and bad), with many multiples of application data retained for archiving and data recovery. Data protection and poor data management practices can result in data sprawl, with many multiples of data duplication, with some estimates as high as 10x levels of redundant copies. By 2025, IDC estimates that approximately 175 zettabytes of data will be created annually, with a compound annual growth rate (CAGR) of around[CE6] 23%. However, only around 2% of data from 2020 was saved into 2021, demonstrating the transient nature of what we create. Of course, some data is filtered and consolidated, rather than being immediately discarded, a process which can\u2019t easily be recreated if that information is lost. Despite the attrition rate, there is constant pressure on IT organisations to store and manage ever greater volumes of data, with an increasingly unpredictable view on whether that information will ever be used again. Many businesses follow the mantra of \u201cretain everything\u201d, which has limits of sustainability and cost. Keeping all data forever also introduces risk for the business, if processes aren\u2019t sufficient to track and inventory content in a timely fashion.","title":"Data Sources"},{"location":"chapter1/#data-lifecycle","text":"What is driving this instability on future data value? For many decades, data had a predictable value over its lifetime. New records in a database, for example, would be frequently accessed when the data was created, then see a decrease in activity over time, before being archived and eventually deleted. This curve was well known and predictable. Credit card processing is a good example, to illustrate data lifecycle. Historically, when a customer made a purchase, the transaction data would be frequently accessed by the both the customer\u2019s and merchant\u2019s bank. Assuming there were no queries with the transaction, the data would be archived after being used to produce a printed statement. Eventually the credit card company would delete the data, perhaps a fixed amount of time after an account was closed or based on whatever regulatory compliance rules required. The original transaction data would quickly lose value as it aged out. Today if a customer uses a credit card, the transaction might get reviewed through a mobile app or online. Banks generally enable customer searching of data a few years into the past. The bank and credit card provider may also use the data for real-time fraud detection, looking at past purchasing trends to determine whether to authorise or decline a transaction. Banks use historical customer data to determine credit limits, both upwards and downwards (many credit card providers will reduce a credit limit to mitigate losses if fraud occurs). Merchants will use credit card data to predict spending habits, for example, offering discount or emailing promotions to prospective buyers. One of the greatest challenges for businesses is to determine when and if data will be useful once past the point of initial creation. Where data used to decline in value over time, the increasing use of analytics means data can have some, as yet, unspecified future value, especially when combined with new data sources. In the early days of computing, businesses could predict that the volume of data within an organisation was generally proportional to the volume of business being done. Today, data is retained on the assumption of future value and an, as yet, unmet need to keep an organisation competitive.","title":"Data Lifecycle"},{"location":"chapter1/#changing-practices","text":"It is true to say that storage infrastructure has never been so cheap, when reviewing a simple $/GB benchmark. Barring one or two blips (due to manufacturing issues, for example the Thailand floods of 2011 ), storage costs have always declined rapidly and consistently over time. The cost of a new modern hard disk drive model, for example, has been static at around $650 when first introduced to the market, irrespective of capacity. This is because the bill of materials (BoM) hasn\u2019t changed much in two decades. Many businesses look at data storage and view the declining price of storage media as an opportunity to retain data forever. However, data management costs and data storage costs are not the same. In addition, the volume of data being stored in the enterprise is growing at a rate that is faster than the reduction in storage media costs when measured over the long term. Data management adds a premium on top of the cost of hardware acquisition. If data is retained on-premises, then there are also facilities costs such as data centre space, power and cooling. In the public cloud, data is charged at a monthly recurring fee for storage and transactional charges for access. The cost of data retention is therefore both a capital and operational cost, depending on where that data is located. Retaining data forever becomes a significant form of technical debt, if not managed correctly. Many organisations have no insight into what\u2019s being stored and where (back to our four application deployment models). If a business doesn\u2019t know what data assets it possesses, how can they adequately be protected? Finally, we should point out the challenges of regulation. In many industries (notably finance and healthcare), extensive regulation exists to ensure data is retained safely for many years past the point of its original use. Businesses can be fined heavily for breaching the rules that govern data management. Outside of industry-specific regulation, GDPR and other country-based constraints place responsibility on businesses to maintain responsible stewardship of personal data, including timely reporting of data loss or data breaches. In addition to rules on data management, governments are starting to introduce measures to ensure businesses can recover data in the event of a malware or ransomware attack. In 2025, the Digital Operational Resilience Act (DORA) became law in the EU, requiring business to improve their digital operational resilience. In 2023, the US government started development of the US Cyber Trust Mark , a labelling programme to ensure IoT devices confirm to a strong cybersecurity framework.","title":"Changing Practices"},{"location":"chapter1/#power-and-responsibility","text":"The value of data provides businesses with competitive advantage, and used correctly has great power. However, at the same time, businesses have a responsibility to use data ethically and within the boundaries put in place by industry regulators. For both strategic and governance reasons, data must be protected from loss or corruption. We will touch on the regulatory issues of data management later in this book, but for now, we need to consider that data protection is an essential business function for all organisations. In the EU, the government introduced the EU AI Act , aimed at creating standards of governance for the use of data within AI-focused applications. The USA also introduced similar AI governance rules, but these were repealed by incoming President Donald Trump during his first days in office[CE7].","title":"Power and Responsibility"},{"location":"chapter1/#backup-versus-archiving","text":"We will come back and discuss the topic in more detail later in this book, but for now we should explain the difference between backup and archiving. Here are two straightforward definitions that explain the difference between the process of backup and archiving. Definition: Data protection is the creation of a point-in-time copy of data that enables recovery of that data at some point in the future, back to the previous time point. Definition: Archiving is the process of moving the primary copy of data out of storage systems to a secondary, (typically cheaper) location, with a view to retaining the data for future reference but not using that data in day-to-day business activities. As we can see, creating a backup results in an additional copy of data being taken, one that represents a static point in time we can return to at a later date. For various reasons, which we will discuss in a moment, it may be necessary to restore all or part of a backup to a previous point, due to issues with the main or current copy. An archive represents a different process, where data is moved out of a primary application and placed elsewhere for future reference, generally on cheaper storage media. Archiving has several benefits. Firstly, it saves on primary disk space that may be an expensive resource. Secondly, it makes applications work faster by reducing the volume of data in search and retrieval functions. The savings made from archiving aren\u2019t just in storage space but can result in lower memory requirements and reduced CPU utilisation. Third, archive data can be stored in a format that provides for better processing, for example, in AI applications or other OLAP applications. Historically, backup has been used as a \u201cpoor quality\u201d archive, with the assumption that data can be retrieved from a backup if needed. Data that would be otherwise archived is simply deleted from the primary system. This approach is risky, as the restore process can be protracted and lengthy. In addition, backups end up being retained much longer than is needed, simply to service the requirement for archived data recovery. In modern IT infrastructure, archived data can easily be moved into a data warehouse or data lake, with specialist tools to provide data insights and analysis. We\u2019ll touch on backup versus archiving in more detail later in this book.","title":"Backup versus Archiving"},{"location":"chapter2/","text":"Why Protect Data? This may seem like an obvious question, why do we need to protect data on IT systems \u2013 isn\u2019t there enough resiliency built into modern hardware and software to ensure data loss doesn\u2019t occur? Before getting to a discussion on technology specifics, there are two important business-related aspects to consider. Data is an asset. Data has always been a business asset, but was generally locked up in sales information, marketing campaign metrics and other traditional forms within structured databases. In the modern enterprise, most (if not all) data assets are digital and cover a wide aspect of business functions, including sales, marketing, customer engagement, product information and research. Much of this data may not be in customer-owned infrastructure. For example, many businesses use Salesforce for customer relationship management, without any understanding on how the data within the platform is stored and protected. Much of newly created data is now coming from automated sources, including interactions with customers, the way products operate and, probably less obviously, third party data types that can be combined with internal data to create new business insights. This includes, for example, the way customers interact within mobile apps or online websites. A large proportion of this data cannot be recreated if lost, as it represents interactions in the real world. Additional examples could include social media reactions to a launch event, sensor measurements on production systems, or user website interactions during a sale event. Like any physical asset, data assets need protection from a range of loss scenarios we\u2019ll discuss in a moment. How businesses account for data assets on the balance sheet is outside the scope of this book, but we expect to see increasingly visible statements on data assets within company accounts. Data has responsibility. While a significant volume of data is anonymous content, a large part of the most critical assets of a business relate to interactions with customers. Some businesses are specifically focused on individuals, such as healthcare or finance. For others selling products, customer data represents personal information including addresses, credit card details and non-fungible data such as social security numbers and dates of birth. Businesses are under legal responsibility to protect and manage PII (personally identifiable information), with hefty fines in place for companies that negligently lose data to exfiltration or deletion. In the EU, GDPR provides for fines of up to 10% of turnover[CME8].. In specific industries, there are regulations in place on managing personal information, such as HIPPA[CME9] in the US and xxxx. These require businesses to reatain data for specific periods of time, for example, in healthcare the lifetime of the patient plus XX years[CE10]. With the risk of significant financial penalties for businesses that fail to adequately protect data, data protection becomes a risk mitigation process and part of the cost of doing business, rather than some unwanted overhead. As another observation, we expect that in the future, businesses will highlight their credentials on data protection practices, enabling customers to make informed choices not just on the products and services sold by the business, but the way in which they are transparent on data governance processes. Data Loss Scenarios As we now know, our data assets are valuable, but what risks are there that may cause data to be lost? We can divide the possible risks into the following scenarios: User Error \u2013 a non-malicious mistake made by an internal employee or system administrator (sometimes colloquially known as \u201cfat fingers\u201d) that results in data loss. For example, deletion of a file or directory. In terms of risk, user errors are quite low on impact but can occur frequently. Some user errors can be much more significant and impactful, depending on the degree of controls in place. Malicious destruction \u2013 scenarios where data is deliberately destroyed or corrupted. The cause of the malicious nature can include internal staff (for example disgruntled employees), data \u201cvandalism\u201d where external actors damage data for no reason (malware), or as a part of a ransomware attack. Ransomware is an area we will discuss in more detail later. Software failure \u2013 all software has inherent bugs that can lead to data corruption. The specific area of failure could be in application code, platform code (such as a database) or in the operating system or storage platform. As software continually changes over time, new bugs can always arise, even if existing software issues are corrected. Infrastructure failure \u2013 hardware is always at risk of failing, either at the component or system level. Hard drives and SSDs fail, sometimes unpredictably. Servers crash and can corrupt data. While we design infrastructure to be resilient, there can always be unforeseen issues with computer hardware that result in system outages and data loss. Ecosystem failure \u2013 more widespread issues can occur at the data centre level. Earthquakes, fire and flood can result in data loss and the inability to access systems. A recent example is the fire at the OVH Strasbourg data centre in 2021. An ecosystem failure could be localised to a rack, a data centre hall or even an entire data centre location. Sadly, as we saw from the September 2001 terror attacks on the USA , having data in a single secondary location such as the alternative twin tower of the World Trade Centre, may not be enough to sufficiently protect data assets. Although modern hardware is more reliable than ever, hardware does fail. Systems are designed for the most part to be fault tolerant, but there\u2019s no way to guarantee that any single piece of computing hardware will have 100% reliability. Software is never perfect and always has bugs, which occasionally lead to data corruption or loss. Some bugs can cause silent corruption of data which might not become apparent until weeks or months later. Even when data centre equipment is running smoothly, outside factors can cause problems. We\u2019ve categorised these as natural disasters, including fire, flooding, or earthquakes. Not all of these are truly natural, of course, such as the explosion and fire at Buncefield[CME11] in the UK or the commercial airliners that crashed into the World Trade[CME12] Centre towers in New York. Some incidents occur internally, but have data centre-wide impacts, such as the fire in the OVH data centre in Strasbourg, France. In the best run systems that have protection from natural disasters, the operational aspects of system operation could also cause data loss. Many users have \u201cfat fingered\u201d their typing, hitting \u201center\u201d before realising the mistake. It\u2019s easy to accidentally drop the wrong database when there are thousands in operation or delete an entire folder of files when meaning to delete a single document. Sometimes data is deleted in good faith, when it\u2019s assumed a spreadsheet, or Word document is no longer needed. It\u2019s only when (for example) a few months\u2019 later that the deleted data is identified as critical for another process that backups become so important. Finally, there\u2019s the issue of malicious damage, which could be from a disgruntled employee or via hacking, either for fun or commercial gain (for example, ransomware). Each of these scenarios can be a challenge to mitigate, as the perpetrators may also target backup systems at the same time. We will cover mitigation strategies for all these scenarios later in this book. Scope What\u2019s clear is that the impacts of data loss can be as small as a minor irritation or as big as an existential crisis for a business. However, do we need to protect all of our data? We will cover service levels in a later chapter, but for now, we would recommend a general rule that by default, all data should be protected, irrespective of how it was created or where it is located. This means being aware of the use of data within every type of business application, including SaaS (software-as-a-service) platforms. Businesses may choose to establish requirements that exclude certain data types from being protected. For example, if a data source is transient and the summarised version is the only copy used for businesses purposes, then it may make sense not to protect the entire data set. If some data is generated from other data (such as reports generated from live customer records), then it may be acceptable not to protect that data if the regeneration time is within the recovery SLA. These are business decisions to make and not the responsibility of IT teams, who only implement the requirements of the data owner. Only the business has insight into the true value of data in the enterprise and what service levels should be applied[CME13] to it. We will discuss this topic further when we look at service levels. What Types of Data Need Protection? What types of data should we be protecting? This question may seem like an obvious one, but the scope of electronic business systems in use today spans much more than the on-premises data centre. If we look back over time, in the 1960s through to the 1980s, almost all data was stored in large and expensive data centres. Most data was also created in the systems that ran in these data centres, either manually entered or through customer portals. The widespread adoption of the Internet in the early 1990s, driven by the development of the World Wide Web, introduced low-cost and ubiquitous networks that facilitated business-to-business (B2B) and business-to-customer (B2C) transactions. Up to this point, proprietary networks, provided expensive communications capabilities, whereas the Internet enabled anyone with a dial-up modem to shop online and businesses with fixed line connections to host websites. From the early 1990s, we saw the widespread use of personal computing, starting with the IBM PC, first released in 1981. Today\u2019s endpoint devices (which can be classified as part of edge computing) include desktops, laptops, tablets and smartphones. In the mid-2000s, cloud computing[CME14] was pioneered by Amazon Web Services, although some online services had existed before then (I personally worked on an online backup service in the mid-1990s that used dial-up connectivity). The capabilities of cloud computing have matured into a multi-billion-dollar industry, with multiple tiers of service providers. All of these systems have data that must be protected. In the last decade, software-as-a-service has become a practical reality, with websites offering IT-based capabilities (data protection being one), but also other business functions such as customer relationship management (CRM), project planning and project management. Collaboration tools (word processing and spreadsheets) are delivered online by Google and Microsoft, while SaaS has also been developed for HR and personnel tasks. Business data now sits across four main areas \u2013 on-premises systems, public cloud systems, SaaS platforms and Edge. Data can be created uniquely in each location, so data protection must encompass them all. There is one key factor that is consistent across all of these deployment models. Data protection is the responsibility of the business and IT department, even where the IT systems storing the data are not directly owned by the business itself. This is particularly important for SaaS and public cloud use cases, where the service levels offered by the platform provider will only cover the restoration of data to the point of a system failure.","title":"2. Why Protect Data?"},{"location":"chapter2/#why-protect-data","text":"This may seem like an obvious question, why do we need to protect data on IT systems \u2013 isn\u2019t there enough resiliency built into modern hardware and software to ensure data loss doesn\u2019t occur? Before getting to a discussion on technology specifics, there are two important business-related aspects to consider. Data is an asset. Data has always been a business asset, but was generally locked up in sales information, marketing campaign metrics and other traditional forms within structured databases. In the modern enterprise, most (if not all) data assets are digital and cover a wide aspect of business functions, including sales, marketing, customer engagement, product information and research. Much of this data may not be in customer-owned infrastructure. For example, many businesses use Salesforce for customer relationship management, without any understanding on how the data within the platform is stored and protected. Much of newly created data is now coming from automated sources, including interactions with customers, the way products operate and, probably less obviously, third party data types that can be combined with internal data to create new business insights. This includes, for example, the way customers interact within mobile apps or online websites. A large proportion of this data cannot be recreated if lost, as it represents interactions in the real world. Additional examples could include social media reactions to a launch event, sensor measurements on production systems, or user website interactions during a sale event. Like any physical asset, data assets need protection from a range of loss scenarios we\u2019ll discuss in a moment. How businesses account for data assets on the balance sheet is outside the scope of this book, but we expect to see increasingly visible statements on data assets within company accounts. Data has responsibility. While a significant volume of data is anonymous content, a large part of the most critical assets of a business relate to interactions with customers. Some businesses are specifically focused on individuals, such as healthcare or finance. For others selling products, customer data represents personal information including addresses, credit card details and non-fungible data such as social security numbers and dates of birth. Businesses are under legal responsibility to protect and manage PII (personally identifiable information), with hefty fines in place for companies that negligently lose data to exfiltration or deletion. In the EU, GDPR provides for fines of up to 10% of turnover[CME8].. In specific industries, there are regulations in place on managing personal information, such as HIPPA[CME9] in the US and xxxx. These require businesses to reatain data for specific periods of time, for example, in healthcare the lifetime of the patient plus XX years[CE10]. With the risk of significant financial penalties for businesses that fail to adequately protect data, data protection becomes a risk mitigation process and part of the cost of doing business, rather than some unwanted overhead. As another observation, we expect that in the future, businesses will highlight their credentials on data protection practices, enabling customers to make informed choices not just on the products and services sold by the business, but the way in which they are transparent on data governance processes.","title":"Why Protect Data?"},{"location":"chapter2/#data-loss-scenarios","text":"As we now know, our data assets are valuable, but what risks are there that may cause data to be lost? We can divide the possible risks into the following scenarios: User Error \u2013 a non-malicious mistake made by an internal employee or system administrator (sometimes colloquially known as \u201cfat fingers\u201d) that results in data loss. For example, deletion of a file or directory. In terms of risk, user errors are quite low on impact but can occur frequently. Some user errors can be much more significant and impactful, depending on the degree of controls in place. Malicious destruction \u2013 scenarios where data is deliberately destroyed or corrupted. The cause of the malicious nature can include internal staff (for example disgruntled employees), data \u201cvandalism\u201d where external actors damage data for no reason (malware), or as a part of a ransomware attack. Ransomware is an area we will discuss in more detail later. Software failure \u2013 all software has inherent bugs that can lead to data corruption. The specific area of failure could be in application code, platform code (such as a database) or in the operating system or storage platform. As software continually changes over time, new bugs can always arise, even if existing software issues are corrected. Infrastructure failure \u2013 hardware is always at risk of failing, either at the component or system level. Hard drives and SSDs fail, sometimes unpredictably. Servers crash and can corrupt data. While we design infrastructure to be resilient, there can always be unforeseen issues with computer hardware that result in system outages and data loss. Ecosystem failure \u2013 more widespread issues can occur at the data centre level. Earthquakes, fire and flood can result in data loss and the inability to access systems. A recent example is the fire at the OVH Strasbourg data centre in 2021. An ecosystem failure could be localised to a rack, a data centre hall or even an entire data centre location. Sadly, as we saw from the September 2001 terror attacks on the USA , having data in a single secondary location such as the alternative twin tower of the World Trade Centre, may not be enough to sufficiently protect data assets. Although modern hardware is more reliable than ever, hardware does fail. Systems are designed for the most part to be fault tolerant, but there\u2019s no way to guarantee that any single piece of computing hardware will have 100% reliability. Software is never perfect and always has bugs, which occasionally lead to data corruption or loss. Some bugs can cause silent corruption of data which might not become apparent until weeks or months later. Even when data centre equipment is running smoothly, outside factors can cause problems. We\u2019ve categorised these as natural disasters, including fire, flooding, or earthquakes. Not all of these are truly natural, of course, such as the explosion and fire at Buncefield[CME11] in the UK or the commercial airliners that crashed into the World Trade[CME12] Centre towers in New York. Some incidents occur internally, but have data centre-wide impacts, such as the fire in the OVH data centre in Strasbourg, France. In the best run systems that have protection from natural disasters, the operational aspects of system operation could also cause data loss. Many users have \u201cfat fingered\u201d their typing, hitting \u201center\u201d before realising the mistake. It\u2019s easy to accidentally drop the wrong database when there are thousands in operation or delete an entire folder of files when meaning to delete a single document. Sometimes data is deleted in good faith, when it\u2019s assumed a spreadsheet, or Word document is no longer needed. It\u2019s only when (for example) a few months\u2019 later that the deleted data is identified as critical for another process that backups become so important. Finally, there\u2019s the issue of malicious damage, which could be from a disgruntled employee or via hacking, either for fun or commercial gain (for example, ransomware). Each of these scenarios can be a challenge to mitigate, as the perpetrators may also target backup systems at the same time. We will cover mitigation strategies for all these scenarios later in this book.","title":"Data Loss Scenarios"},{"location":"chapter2/#scope","text":"What\u2019s clear is that the impacts of data loss can be as small as a minor irritation or as big as an existential crisis for a business. However, do we need to protect all of our data? We will cover service levels in a later chapter, but for now, we would recommend a general rule that by default, all data should be protected, irrespective of how it was created or where it is located. This means being aware of the use of data within every type of business application, including SaaS (software-as-a-service) platforms. Businesses may choose to establish requirements that exclude certain data types from being protected. For example, if a data source is transient and the summarised version is the only copy used for businesses purposes, then it may make sense not to protect the entire data set. If some data is generated from other data (such as reports generated from live customer records), then it may be acceptable not to protect that data if the regeneration time is within the recovery SLA. These are business decisions to make and not the responsibility of IT teams, who only implement the requirements of the data owner. Only the business has insight into the true value of data in the enterprise and what service levels should be applied[CME13] to it. We will discuss this topic further when we look at service levels.","title":"Scope"},{"location":"chapter2/#what-types-of-data-need-protection","text":"What types of data should we be protecting? This question may seem like an obvious one, but the scope of electronic business systems in use today spans much more than the on-premises data centre. If we look back over time, in the 1960s through to the 1980s, almost all data was stored in large and expensive data centres. Most data was also created in the systems that ran in these data centres, either manually entered or through customer portals. The widespread adoption of the Internet in the early 1990s, driven by the development of the World Wide Web, introduced low-cost and ubiquitous networks that facilitated business-to-business (B2B) and business-to-customer (B2C) transactions. Up to this point, proprietary networks, provided expensive communications capabilities, whereas the Internet enabled anyone with a dial-up modem to shop online and businesses with fixed line connections to host websites. From the early 1990s, we saw the widespread use of personal computing, starting with the IBM PC, first released in 1981. Today\u2019s endpoint devices (which can be classified as part of edge computing) include desktops, laptops, tablets and smartphones. In the mid-2000s, cloud computing[CME14] was pioneered by Amazon Web Services, although some online services had existed before then (I personally worked on an online backup service in the mid-1990s that used dial-up connectivity). The capabilities of cloud computing have matured into a multi-billion-dollar industry, with multiple tiers of service providers. All of these systems have data that must be protected. In the last decade, software-as-a-service has become a practical reality, with websites offering IT-based capabilities (data protection being one), but also other business functions such as customer relationship management (CRM), project planning and project management. Collaboration tools (word processing and spreadsheets) are delivered online by Google and Microsoft, while SaaS has also been developed for HR and personnel tasks. Business data now sits across four main areas \u2013 on-premises systems, public cloud systems, SaaS platforms and Edge. Data can be created uniquely in each location, so data protection must encompass them all. There is one key factor that is consistent across all of these deployment models. Data protection is the responsibility of the business and IT department, even where the IT systems storing the data are not directly owned by the business itself. This is particularly important for SaaS and public cloud use cases, where the service levels offered by the platform provider will only cover the restoration of data to the point of a system failure.","title":"What Types of Data Need Protection?"},{"location":"chapter3/","text":"Defining Data Protection Now that we\u2019re clear why data needs protecting, we should clarify exactly what is meant by data protection and how that definition has changed in a modern context. Looking back 40 years or more, the term data protection was synonymous with backup and restore. If data was lost, then a copy could be retrieved from the backup. This process was acceptable when most of our computing activities were batch processes. Early computing in the 1950s and 1960s focused on the asynchronous analysis of data fed into a centralised mainframe-style system. There was rarely an \u201conline\u201d component, what we generally refer to as OLTP or Online Transaction Processing. IBM, for example, only introduced CICS ( Customer Information Control System ), an OLTP middleware system, in 1969. With much of the processing not directly user-dependent, recovering from a backup was a reasonable recovery strategy, especially where backups could be taken (or checkpointed) in line with the start and end of batch processing work. Many IT professionals from the 1970s and 1980s will be familiar with rerunning a job from a particular step or in its entirety. We should also remember that much of the data in computer systems of the late 20th century would have been manually entered. If a restore returned a database or other system to a state the day or week before, then theoretically, much of that data could be re-entered and bring the system back up to date. As we highlighted earlier, that luxury no longer exists, as we will now discuss in more detail. Online Day Before the advent of the World Wide Web, most computing systems would be focused on the delivery of services during the online day, or between the hours of (approximately) 9am to 5pm and Monday to Friday, generally in a single time zone. Two factors changed the online day into 24/7 operations. The first was the adoption of the World Wide Web, which enabled both B2B (business to business) and B2C (business to customer) operations. The late 1990s saw the rise of new technology companies such as Amazon.com and a swathe of early companies riding the first dotcom boom. The second factor was globalisation. Although Amazon launched in the US first, it was still possible to ship from the US to the rest of the world. Many websites offered products that weren\u2019t physical and so needed no shipping, for example music or software. As the use of the Internet and the World Wide Web increased, businesses moved from the traditional 9-5 operation to being global 24/7 enterprises, where downtime and outages would result in loss of business and reputational damage. Now, simple backup and restore wasn\u2019t enough to keep IT systems running within the service levels needed to do business. The World Wide Web The growth of the Internet introduced one other major issue for data protection. In the legacy model of tightly controlled data centres and mainframes, manual data entry played a big part of the data creation process. The Internet resulted in systems that could handle thousands and eventually millions of concurrent transactions. Some of this data might be commercial transactions \u2013 customer orders for products and services \u2013 while other data represented interactions with the system, such as popular pages and search requests. This data is incredibly valuable to an online merchant, as it allows a website to direct the customer with more targeted ads. Amazon.com\u2019s advertising revenue in Q3 FY2024, for example, was worth $14.3 billion or around 9% of turnover13. Similarly, Google, Meta and Microsoft all depend on user behaviour tracking to optimise their platforms. User-generated data also exists outside the commercial environment. IMDb, for example, founded in 1990, started as a website for fans to share film and movie information, initially as a Usenet group and then as a dedicated website. The IMDb database represents millions of hours of user content creation (in 2022, the database contained details of 10.1 million titles with 11.5 million person records and 83 million registered users). If the IMDb database was lost, it would be almost impossible to recreate in its current form. Similarly, we can look at Stack Overflow, GitHub, Reddit, and thousands if not millions of other websites that have evolved over time and could never be rebuilt without backups. As the proliferation of IT in our daily lives has continued, whether that is for commerce, banking or entertainment, we now depend on the ability for data protection to recover data at any time and in as timely a fashion as possible. How is that achieved, with such a complex web of infrastructure that spans our four categories? We will start by looking at the concept of a data protection hierarchy and then examine the metrics used to quantify the requirements of a data protection process. Protection Hierarchy We can define a data protection hierarchy as a system that uses multiple techniques and processes to ensure data is available 24/7. We will expand on these later in much more detail, but briefly introduce them now to explain why simple backup and restore is no longer enough to ensure continuous operations. BC/DR \u2013 An acronym for two terms \u2013 business continuity and disaster recovery. Ultimately, all data protection and systems management functions work towards maintaining business continuity, which doesn\u2019t just refer to IT systems. Disaster Recovery defines the process of recovering from a major incident, such as the loss of a data centre, a ransomware attack or major network outage. DLP \u2013 Data Leakage Protection \u2013 a relatively new term that describes the requirement to ensure no data is lost to external exfiltration. We will look at DLP when covering ransomware, but for now, think of the need to ensure that sensitive data within the business doesn\u2019t get into the hands of competitors, bad actors looking to blackmail an organisation or be leaked on the Internet to cause reputational damage. Data Protection \u2013 the process of protecting data assets from loss or corruption by systems failure, natural disasters or human error. The protection of data can be real-time or point-in-time, depending on the nature of the loss and recovery required. There are many techniques employed to protect data, not all of which need or should be point-in-time solutions. Backup/Restore \u2013 the process of taking a point-in-time copy of data and using that data to recover an application or computer system to a previously known good state. Backup and restore doesn\u2019t need to restore an entire application and could be used to perform partial recovery, such as a deleted or corrupted file. Recovery vs Continuity We briefly mentioned the concept of business continuity as a process of maintaining the availability of computer systems for ongoing access. Ideally, we want to ensure data loss is mitigated or avoided altogether. As a result, computer systems design will include resiliency capabilities that protect data, but don\u2019t require a recovery process. We will discuss metrics in a moment, but clearly it is more desirable to provide continued access to data than lose it and need to recover. The difference between the two is obvious and is explained by the downtime or outage experienced by a computer system or application. Building resilience into a system results in additional cost, as the usual process of achieving high availability is redundant hardware and more complex software. There is a clear trade-off when building applications as to the level of resilience required. A banking application will generally want 100% uptime (or as close as possible), with no loss of data. A social media platform or blog website may desire 100% availability, but can live with some downtime and even some data loss. Alternative Uses So far, we\u2019ve covered data protection as a mechanism to restore operations from either systems failure, natural disasters or human error. However, data protected by backup systems can be used for alternative processes. Many businesses, for example, use point-in-time backups as the source of test/development data or in place of an archiving system. The use of live production data for testing and development purposes needs to be considered carefully. In most industries there are guidelines on the use of personally identifiable information (PII) that must be followed. In certain jurisdictions, regulations such as the EU GDPR, the CCPA (California Consumer Privacy Act) and UK\u2019s Data Protection Act can result in significant penalties for the misuse of PII. As a result, PII data being used in test environments should be tokenised to remove sensitive information.i Anonymisation is good practice, as test data will generally have a lower standard of security applied, compared to production data. This includes both the security of systems and those people within a company who should normally have access to what could be sensitive information. As a best practice, no data should be accessible by development teams without having gone through the tokenisation or anonymisation process. The use of backups as an archive also presents problems to the business and IT departments. Typically, an archive is an extract of inactive customer or business data that needs to be retained for business purposes but isn\u2019t in active use. The archiving of data out of live production systems has positive benefits to live operations, reducing the volume of daily backups and reducing the footprint (both storage and CPU/memory requirements) of databases and other systems to just the data being processed. Reducing the size of a production footprint by archiving records also improves recovery time, if data is lost. Archived data can also be placed behind logical or physical air gaps, reducing the risk of encryption by ransomware. In some businesses, archiving is implemented simply by the long-term retention of backups. This process is problematic for many reasons. Firstly, the size of backups continues to grow over time, including those retained for archive. This creates \u201cbackup sprawl\u201d, with many more copies of data retained than is strictly necessary. The storage industry addressed this problem with data deduplication and deduplicating backup appliances. Although this was a novel and innovative approach, it doesn\u2019t solve the issue that any restore from backup could require recovering an entire dataset (especially with structured databases), elongating the restore time for primary recovery and for accessing data in an archiving process. Second, data that is retained long term will continue to contain records for customers or individuals that are no longer actively engaging with the business. Under GDPR rules, for example, those people could invoke the \u201cright to be forgotten\u201d (RTBF). Removing individual data records from backups is almost impossible to achieve, requiring businesses to keep secondary information on those individuals, ensuring that should data be restored from a long-term backup, the records relating to RTBF customers is subsequently removed. This creates a paradox in that businesses must retain data on customers who have the right not to have their data retained. Getting the RTBF process wrong could result in big fines for businesses. This makes it more logical to build better processes and data architectures that implement robust archiving, rather than relying on retained backups. Service Level Agreement and Objectives Earlier in this book we touched on the topic of service levels with respect to data protection, both in the backup and restore process. As we look at what service levels and service objectives should look like, we need to distinguish between the responsibilities of data owners and data managers in the data protection sphere. A data owner, such as a line of business, or business unit should be the decision maker when deciding what data needs to be protected. The data and application owner also needs to determine the frequency and retention of backups, with help from the IT department (more on this in a moment). Data service levels lie with the business because only the data owner knows the value of their data. In addition, the data owner knows the turnover and \u201cvelocity\u201d of that data in terms of the rate of creation and overall turnover. As a third metric, the business data owner understands the implications of data inaccessibility or downtime. This cost translates directly into the effort that needs to be placed on restoring accessibility, including data recovery time. Data managers, such as IT teams responsible for data protection, will work with data owners and define a framework in which data protection processes are applied. A framework typically consists of both service level agreements and objectives. A service level agreement (SLA) is simply a formal or informal contract between an application or data owner and a service provider to meet a certain level of service when delivering infrastructure or services to support an application. Service levels cover uptime (availability), performance (application response times and throughput capability) and for the purposes of our discussion, backup frequency, backup elapsed times, and recovery times in the case of an outage. Note that a service level agreement could be in place with a third-party supplier (like an MSP or cloud platform) and be in the form of a legal contract that includes financial penalties when service levels are missed. Public cloud vendors, for example, offer SLAs on the performance and uptime of cloud services, with service credits offered when the levels are not met. Service level objectives generally have less formal definitions and represent targets rather than binding agreements. Data protection typically measures two main targets for recovery of data. These are Recovery Point Objective and Recovery Time Objective , defined as follows: Recovery Point Objective \u2013 a measure of how current data will be at the time of recovery, measured in time (minutes, hours or days) and is essentially a measure of how much data loss an application can tolerate. For example, a web application may accept an RPO=15 minutes, meaning data is recovered to a point 15 minutes previously, with some loss of data. A banking application will almost certainly only tolerate RPO=0, meaning no data loss is tolerated at all. Recovery Time Objective \u2013 the time taken to recover an application back to operational state, in the event of a failure. Ideally, applications would be restored back to a live operational state instantly, but that\u2019s not practical or cost effective for every application type. Therefore, some degree of delay is inevitable, as data is restored from backups into live systems. In general, business owners will want and expect the lowest RPO and RTO possible. It is the responsibility of the IT teams to work with the business and data owners to establish acceptable recovery times and recovery points that are practical to deliver within a reasonable budget. All the metrics established for data protection will be dependent on the volume of data to be protected, which has a direct implication of the cost of protection and the infrastructure needed to deliver it. For example, if backup takes one hour to protect an application, then a 15-minute recovery point objective is not going to work. However, many of the challenges around backup times can be mitigated, depending on the infrastructure involved. Similarly, backup and restore times can be asymmetrical. Deduplicating appliances, for example, typically ingested backup data much quicker than it could be restored, creating an imbalance in the design of infrastructure that has cost and scaling implications. Both RTO and RPO relate to the recovery of data after a loss has occurred. However, it is just as important to specify service level objectives on how frequently data is protected, how many copies are retained and for what length of time. The frequency of backup is, of course, directly related to the recovery point objective. If data needs to be restored with a maximum of 15 minutes RPO, then data needs to be protected at least every 15 minutes, for example. Historically, legacy data protection platforms have required the backup administrator to translate the myriad service level objectives into backup scheduling and prioritisation. Modern data protection solutions obfuscate the need for manual scheduling by defining protection policies. This method of operation reduces the manual overhead on backup scheduling and adjustments, while providing for the measurement of achieved service level success compared to the desired service level objectives.","title":"3. Defining Data Protection"},{"location":"chapter3/#defining-data-protection","text":"Now that we\u2019re clear why data needs protecting, we should clarify exactly what is meant by data protection and how that definition has changed in a modern context. Looking back 40 years or more, the term data protection was synonymous with backup and restore. If data was lost, then a copy could be retrieved from the backup. This process was acceptable when most of our computing activities were batch processes. Early computing in the 1950s and 1960s focused on the asynchronous analysis of data fed into a centralised mainframe-style system. There was rarely an \u201conline\u201d component, what we generally refer to as OLTP or Online Transaction Processing. IBM, for example, only introduced CICS ( Customer Information Control System ), an OLTP middleware system, in 1969. With much of the processing not directly user-dependent, recovering from a backup was a reasonable recovery strategy, especially where backups could be taken (or checkpointed) in line with the start and end of batch processing work. Many IT professionals from the 1970s and 1980s will be familiar with rerunning a job from a particular step or in its entirety. We should also remember that much of the data in computer systems of the late 20th century would have been manually entered. If a restore returned a database or other system to a state the day or week before, then theoretically, much of that data could be re-entered and bring the system back up to date. As we highlighted earlier, that luxury no longer exists, as we will now discuss in more detail.","title":"Defining Data Protection"},{"location":"chapter3/#online-day","text":"Before the advent of the World Wide Web, most computing systems would be focused on the delivery of services during the online day, or between the hours of (approximately) 9am to 5pm and Monday to Friday, generally in a single time zone. Two factors changed the online day into 24/7 operations. The first was the adoption of the World Wide Web, which enabled both B2B (business to business) and B2C (business to customer) operations. The late 1990s saw the rise of new technology companies such as Amazon.com and a swathe of early companies riding the first dotcom boom. The second factor was globalisation. Although Amazon launched in the US first, it was still possible to ship from the US to the rest of the world. Many websites offered products that weren\u2019t physical and so needed no shipping, for example music or software. As the use of the Internet and the World Wide Web increased, businesses moved from the traditional 9-5 operation to being global 24/7 enterprises, where downtime and outages would result in loss of business and reputational damage. Now, simple backup and restore wasn\u2019t enough to keep IT systems running within the service levels needed to do business.","title":"Online Day"},{"location":"chapter3/#the-world-wide-web","text":"The growth of the Internet introduced one other major issue for data protection. In the legacy model of tightly controlled data centres and mainframes, manual data entry played a big part of the data creation process. The Internet resulted in systems that could handle thousands and eventually millions of concurrent transactions. Some of this data might be commercial transactions \u2013 customer orders for products and services \u2013 while other data represented interactions with the system, such as popular pages and search requests. This data is incredibly valuable to an online merchant, as it allows a website to direct the customer with more targeted ads. Amazon.com\u2019s advertising revenue in Q3 FY2024, for example, was worth $14.3 billion or around 9% of turnover13. Similarly, Google, Meta and Microsoft all depend on user behaviour tracking to optimise their platforms. User-generated data also exists outside the commercial environment. IMDb, for example, founded in 1990, started as a website for fans to share film and movie information, initially as a Usenet group and then as a dedicated website. The IMDb database represents millions of hours of user content creation (in 2022, the database contained details of 10.1 million titles with 11.5 million person records and 83 million registered users). If the IMDb database was lost, it would be almost impossible to recreate in its current form. Similarly, we can look at Stack Overflow, GitHub, Reddit, and thousands if not millions of other websites that have evolved over time and could never be rebuilt without backups. As the proliferation of IT in our daily lives has continued, whether that is for commerce, banking or entertainment, we now depend on the ability for data protection to recover data at any time and in as timely a fashion as possible. How is that achieved, with such a complex web of infrastructure that spans our four categories? We will start by looking at the concept of a data protection hierarchy and then examine the metrics used to quantify the requirements of a data protection process.","title":"The World Wide Web"},{"location":"chapter3/#protection-hierarchy","text":"We can define a data protection hierarchy as a system that uses multiple techniques and processes to ensure data is available 24/7. We will expand on these later in much more detail, but briefly introduce them now to explain why simple backup and restore is no longer enough to ensure continuous operations. BC/DR \u2013 An acronym for two terms \u2013 business continuity and disaster recovery. Ultimately, all data protection and systems management functions work towards maintaining business continuity, which doesn\u2019t just refer to IT systems. Disaster Recovery defines the process of recovering from a major incident, such as the loss of a data centre, a ransomware attack or major network outage. DLP \u2013 Data Leakage Protection \u2013 a relatively new term that describes the requirement to ensure no data is lost to external exfiltration. We will look at DLP when covering ransomware, but for now, think of the need to ensure that sensitive data within the business doesn\u2019t get into the hands of competitors, bad actors looking to blackmail an organisation or be leaked on the Internet to cause reputational damage. Data Protection \u2013 the process of protecting data assets from loss or corruption by systems failure, natural disasters or human error. The protection of data can be real-time or point-in-time, depending on the nature of the loss and recovery required. There are many techniques employed to protect data, not all of which need or should be point-in-time solutions. Backup/Restore \u2013 the process of taking a point-in-time copy of data and using that data to recover an application or computer system to a previously known good state. Backup and restore doesn\u2019t need to restore an entire application and could be used to perform partial recovery, such as a deleted or corrupted file.","title":"Protection Hierarchy"},{"location":"chapter3/#recovery-vs-continuity","text":"We briefly mentioned the concept of business continuity as a process of maintaining the availability of computer systems for ongoing access. Ideally, we want to ensure data loss is mitigated or avoided altogether. As a result, computer systems design will include resiliency capabilities that protect data, but don\u2019t require a recovery process. We will discuss metrics in a moment, but clearly it is more desirable to provide continued access to data than lose it and need to recover. The difference between the two is obvious and is explained by the downtime or outage experienced by a computer system or application. Building resilience into a system results in additional cost, as the usual process of achieving high availability is redundant hardware and more complex software. There is a clear trade-off when building applications as to the level of resilience required. A banking application will generally want 100% uptime (or as close as possible), with no loss of data. A social media platform or blog website may desire 100% availability, but can live with some downtime and even some data loss.","title":"Recovery vs Continuity"},{"location":"chapter3/#alternative-uses","text":"So far, we\u2019ve covered data protection as a mechanism to restore operations from either systems failure, natural disasters or human error. However, data protected by backup systems can be used for alternative processes. Many businesses, for example, use point-in-time backups as the source of test/development data or in place of an archiving system. The use of live production data for testing and development purposes needs to be considered carefully. In most industries there are guidelines on the use of personally identifiable information (PII) that must be followed. In certain jurisdictions, regulations such as the EU GDPR, the CCPA (California Consumer Privacy Act) and UK\u2019s Data Protection Act can result in significant penalties for the misuse of PII. As a result, PII data being used in test environments should be tokenised to remove sensitive information.i Anonymisation is good practice, as test data will generally have a lower standard of security applied, compared to production data. This includes both the security of systems and those people within a company who should normally have access to what could be sensitive information. As a best practice, no data should be accessible by development teams without having gone through the tokenisation or anonymisation process. The use of backups as an archive also presents problems to the business and IT departments. Typically, an archive is an extract of inactive customer or business data that needs to be retained for business purposes but isn\u2019t in active use. The archiving of data out of live production systems has positive benefits to live operations, reducing the volume of daily backups and reducing the footprint (both storage and CPU/memory requirements) of databases and other systems to just the data being processed. Reducing the size of a production footprint by archiving records also improves recovery time, if data is lost. Archived data can also be placed behind logical or physical air gaps, reducing the risk of encryption by ransomware. In some businesses, archiving is implemented simply by the long-term retention of backups. This process is problematic for many reasons. Firstly, the size of backups continues to grow over time, including those retained for archive. This creates \u201cbackup sprawl\u201d, with many more copies of data retained than is strictly necessary. The storage industry addressed this problem with data deduplication and deduplicating backup appliances. Although this was a novel and innovative approach, it doesn\u2019t solve the issue that any restore from backup could require recovering an entire dataset (especially with structured databases), elongating the restore time for primary recovery and for accessing data in an archiving process. Second, data that is retained long term will continue to contain records for customers or individuals that are no longer actively engaging with the business. Under GDPR rules, for example, those people could invoke the \u201cright to be forgotten\u201d (RTBF). Removing individual data records from backups is almost impossible to achieve, requiring businesses to keep secondary information on those individuals, ensuring that should data be restored from a long-term backup, the records relating to RTBF customers is subsequently removed. This creates a paradox in that businesses must retain data on customers who have the right not to have their data retained. Getting the RTBF process wrong could result in big fines for businesses. This makes it more logical to build better processes and data architectures that implement robust archiving, rather than relying on retained backups.","title":"Alternative Uses"},{"location":"chapter3/#service-level-agreement-and-objectives","text":"Earlier in this book we touched on the topic of service levels with respect to data protection, both in the backup and restore process. As we look at what service levels and service objectives should look like, we need to distinguish between the responsibilities of data owners and data managers in the data protection sphere. A data owner, such as a line of business, or business unit should be the decision maker when deciding what data needs to be protected. The data and application owner also needs to determine the frequency and retention of backups, with help from the IT department (more on this in a moment). Data service levels lie with the business because only the data owner knows the value of their data. In addition, the data owner knows the turnover and \u201cvelocity\u201d of that data in terms of the rate of creation and overall turnover. As a third metric, the business data owner understands the implications of data inaccessibility or downtime. This cost translates directly into the effort that needs to be placed on restoring accessibility, including data recovery time. Data managers, such as IT teams responsible for data protection, will work with data owners and define a framework in which data protection processes are applied. A framework typically consists of both service level agreements and objectives. A service level agreement (SLA) is simply a formal or informal contract between an application or data owner and a service provider to meet a certain level of service when delivering infrastructure or services to support an application. Service levels cover uptime (availability), performance (application response times and throughput capability) and for the purposes of our discussion, backup frequency, backup elapsed times, and recovery times in the case of an outage. Note that a service level agreement could be in place with a third-party supplier (like an MSP or cloud platform) and be in the form of a legal contract that includes financial penalties when service levels are missed. Public cloud vendors, for example, offer SLAs on the performance and uptime of cloud services, with service credits offered when the levels are not met. Service level objectives generally have less formal definitions and represent targets rather than binding agreements. Data protection typically measures two main targets for recovery of data. These are Recovery Point Objective and Recovery Time Objective , defined as follows: Recovery Point Objective \u2013 a measure of how current data will be at the time of recovery, measured in time (minutes, hours or days) and is essentially a measure of how much data loss an application can tolerate. For example, a web application may accept an RPO=15 minutes, meaning data is recovered to a point 15 minutes previously, with some loss of data. A banking application will almost certainly only tolerate RPO=0, meaning no data loss is tolerated at all. Recovery Time Objective \u2013 the time taken to recover an application back to operational state, in the event of a failure. Ideally, applications would be restored back to a live operational state instantly, but that\u2019s not practical or cost effective for every application type. Therefore, some degree of delay is inevitable, as data is restored from backups into live systems. In general, business owners will want and expect the lowest RPO and RTO possible. It is the responsibility of the IT teams to work with the business and data owners to establish acceptable recovery times and recovery points that are practical to deliver within a reasonable budget. All the metrics established for data protection will be dependent on the volume of data to be protected, which has a direct implication of the cost of protection and the infrastructure needed to deliver it. For example, if backup takes one hour to protect an application, then a 15-minute recovery point objective is not going to work. However, many of the challenges around backup times can be mitigated, depending on the infrastructure involved. Similarly, backup and restore times can be asymmetrical. Deduplicating appliances, for example, typically ingested backup data much quicker than it could be restored, creating an imbalance in the design of infrastructure that has cost and scaling implications. Both RTO and RPO relate to the recovery of data after a loss has occurred. However, it is just as important to specify service level objectives on how frequently data is protected, how many copies are retained and for what length of time. The frequency of backup is, of course, directly related to the recovery point objective. If data needs to be restored with a maximum of 15 minutes RPO, then data needs to be protected at least every 15 minutes, for example. Historically, legacy data protection platforms have required the backup administrator to translate the myriad service level objectives into backup scheduling and prioritisation. Modern data protection solutions obfuscate the need for manual scheduling by defining protection policies. This method of operation reduces the manual overhead on backup scheduling and adjustments, while providing for the measurement of achieved service level success compared to the desired service level objectives.","title":"Service Level Agreement and Objectives"},{"location":"chapter4/","text":"ETIL What are the mechanics of a data protection process? Many readers may be familiar with the term \u201cETL\u201d, a common standard in data pipelines which stands for Extract, Transform and Load. Data processing techniques, such as those which build data warehouses, extract data from production systems, transform them in some way (such as changing the structure into XML or JSON, or removing personal information) and then store that data in another system for further use (the load step). ETL can also be used for data cleaning, where the quality of source data is poor or needs additional metadata or other content to supplement it. An example could be adding postcodes (zip codes) to addresses, or validating date of birth information is in the correct format and a valid date. In this book we use ETIL to describe the process in which data is secured as a backup for future use in recovery. The additional step represented by the letter \u201cI\u201d stands for \u201cindexing\u201d, essentially ensuring that the data being protected can be easily searched in the future. In essence, data protection is like an ETL process, but with that one additional step, which provides for the future searching of backup information. Why do we need these four processes in data protection? Let\u2019s look at each of them in a little more detail. Extract As we\u2019ve already explained, backup is the process of taking a point-in-time copy of data as an insurance policy for some future incident. Essentially, we extract a copy of data from our primary applications and systems. Exactly how this is done depends on the data and the platform. For example, data on a shared storage array could be extracted via a snapshot copy; data in a database could be extracted using a SQL query; data in a virtual server platform could be extracted via an advertised API. Each of these processes will be tailored to the platform and the data. Using the server virtualisation platform as an example, VMware vSphere offers a specific API to extract changed data at a block level for individual virtual machines. VADP (VMware vSphere Storage APIs \u2013 Data Protection) provides a stream of changed blocks separately for each virtual machine, which is more efficient than, taking snapshots within the virtual machine or copying data at the datastore level. Data extraction must be non-invasive (as much as possible), create a consistent version of data, while being obtained in a form that can be usefully used later for recovery. Transform Once a backup solution has extracted data from a primary application, that data will go through some form of transformation. The transform step is used to optimise data before it is saved for future use. This could mean deduplicating the content (if identical data already exists on the backup system), parsing the data to extract only the content to be stored (from a snapshot, for example), or discarding data that hasn\u2019t been changed since the last backup cycle. The transformation process may also put data into a different format that is optimised for long-term storage. This may mean aggregation into large chunks of data for storage on tape or in the public cloud. Index Data in backups is retained purely to facilitate recovery. As a result, data protection systems need to be able to retrieve that data at some future point. This requirement has two processing aspects. Firstly, an index of backup content needs to know where to find the backup copy. Historically, this might mean a tape in the data centre or stored offsite at a 3rd party management company. Today, backup data could be stored on an object store in the public cloud. The second reason for indexing is to find data that may no longer have a primary copy. For example, imagine refactoring an application onto the public cloud from an on-premises virtual server. Although the application functionality remains the same, the data may now be in a virtual cloud instance with a different name, location and running on an updated application (such as a structured database). If data from before the refactoring is needed, then some system of record is required to know that prior to the refactoring, the data and application existed elsewhere within the \u201cvirtual\u201d data centre that could span on-premises and multiple public clouds. Without a system of record, any recovery would require site knowledge that is easily lost over time. Load Once data has been extracted from a primary application, the transformed copy must be stored somewhere. Unlike primary copies of data, backup copies (sometimes referred to as secondary copies) are generally stored on storage media suitable for long-term and low-cost retention. Historically this has meant sequential access technology like magnetic tape, although modern backup solutions use a mix of media types to facilitate quick restores of the most recently protected data. The format for long-term retention of backups is an important aspect to consider, when building a data protection strategy. We will dig into this topic in much more detail later. However, it is worth highlighting that many businesses fail to implement a strategic approach to long-term data backup management, leaving a legacy of ageing tapes that may contain unknown content. The answer to this problem is, in many instances, to \u201ckick the can down the road\u201d and leave the problem to some future date. That strategy isn\u2019t always the right approach, as it introduces the risk of compliance and regulatory issues that could include financial penalties.","title":"4. ETIL"},{"location":"chapter4/#etil","text":"What are the mechanics of a data protection process? Many readers may be familiar with the term \u201cETL\u201d, a common standard in data pipelines which stands for Extract, Transform and Load. Data processing techniques, such as those which build data warehouses, extract data from production systems, transform them in some way (such as changing the structure into XML or JSON, or removing personal information) and then store that data in another system for further use (the load step). ETL can also be used for data cleaning, where the quality of source data is poor or needs additional metadata or other content to supplement it. An example could be adding postcodes (zip codes) to addresses, or validating date of birth information is in the correct format and a valid date. In this book we use ETIL to describe the process in which data is secured as a backup for future use in recovery. The additional step represented by the letter \u201cI\u201d stands for \u201cindexing\u201d, essentially ensuring that the data being protected can be easily searched in the future. In essence, data protection is like an ETL process, but with that one additional step, which provides for the future searching of backup information. Why do we need these four processes in data protection? Let\u2019s look at each of them in a little more detail.","title":"ETIL"},{"location":"chapter4/#extract","text":"As we\u2019ve already explained, backup is the process of taking a point-in-time copy of data as an insurance policy for some future incident. Essentially, we extract a copy of data from our primary applications and systems. Exactly how this is done depends on the data and the platform. For example, data on a shared storage array could be extracted via a snapshot copy; data in a database could be extracted using a SQL query; data in a virtual server platform could be extracted via an advertised API. Each of these processes will be tailored to the platform and the data. Using the server virtualisation platform as an example, VMware vSphere offers a specific API to extract changed data at a block level for individual virtual machines. VADP (VMware vSphere Storage APIs \u2013 Data Protection) provides a stream of changed blocks separately for each virtual machine, which is more efficient than, taking snapshots within the virtual machine or copying data at the datastore level. Data extraction must be non-invasive (as much as possible), create a consistent version of data, while being obtained in a form that can be usefully used later for recovery.","title":"Extract"},{"location":"chapter4/#transform","text":"Once a backup solution has extracted data from a primary application, that data will go through some form of transformation. The transform step is used to optimise data before it is saved for future use. This could mean deduplicating the content (if identical data already exists on the backup system), parsing the data to extract only the content to be stored (from a snapshot, for example), or discarding data that hasn\u2019t been changed since the last backup cycle. The transformation process may also put data into a different format that is optimised for long-term storage. This may mean aggregation into large chunks of data for storage on tape or in the public cloud.","title":"Transform"},{"location":"chapter4/#index","text":"Data in backups is retained purely to facilitate recovery. As a result, data protection systems need to be able to retrieve that data at some future point. This requirement has two processing aspects. Firstly, an index of backup content needs to know where to find the backup copy. Historically, this might mean a tape in the data centre or stored offsite at a 3rd party management company. Today, backup data could be stored on an object store in the public cloud. The second reason for indexing is to find data that may no longer have a primary copy. For example, imagine refactoring an application onto the public cloud from an on-premises virtual server. Although the application functionality remains the same, the data may now be in a virtual cloud instance with a different name, location and running on an updated application (such as a structured database). If data from before the refactoring is needed, then some system of record is required to know that prior to the refactoring, the data and application existed elsewhere within the \u201cvirtual\u201d data centre that could span on-premises and multiple public clouds. Without a system of record, any recovery would require site knowledge that is easily lost over time.","title":"Index"},{"location":"chapter4/#load","text":"Once data has been extracted from a primary application, the transformed copy must be stored somewhere. Unlike primary copies of data, backup copies (sometimes referred to as secondary copies) are generally stored on storage media suitable for long-term and low-cost retention. Historically this has meant sequential access technology like magnetic tape, although modern backup solutions use a mix of media types to facilitate quick restores of the most recently protected data. The format for long-term retention of backups is an important aspect to consider, when building a data protection strategy. We will dig into this topic in much more detail later. However, it is worth highlighting that many businesses fail to implement a strategic approach to long-term data backup management, leaving a legacy of ageing tapes that may contain unknown content. The answer to this problem is, in many instances, to \u201ckick the can down the road\u201d and leave the problem to some future date. That strategy isn\u2019t always the right approach, as it introduces the risk of compliance and regulatory issues that could include financial penalties.","title":"Load"},{"location":"chapter5/","text":"A Short History of Data Protection Data protection has been a key feature of computer systems since the first commercial computers entered production in 1951. For at least two decades, backup and recovery of data was a bespoke solution implemented by systems programmers, rather than a product which could be bought and installed on the system. Part of the challenge for system operators at that time was the lack of suitable offline media. UNISERVO, a tape system based on heavy steel tape, was introduced for the UNIVAC I in 1951. IBM introduced its first tape solution, the IBM 726, with the IBM 701 in 1952. Reel-to-Reel tape persisted as the standard medium, based on magnetic coated film, until the mid-1980s, with the introduction of the IBM 3480 tape cartridge. On the IBM mainframe, Hierarchical Storage Manager was introduced by IBM on the MVS operating system and System/370 platform in 1977. Version 2 was renamed DFHSM (Data Facility Hierarchical Storage Manager) and eventually DFSMShsm with the introduction of system-managed storage and the Storage Management Subsystem (DFSMS) in the late 1980s. Elsewhere, Unix systems generally relied on native commands, including tar, introduced in Unix Version 7 in 1979. The tar utility was preceded by tp and tap commands. Other utilities such as cp and cpio offered the capability to transfer data to and from tape. Many of the data protection solutions we know today had origins in bespoke or custom software tools. BackupExec derived from tape streaming software developed by Maynard Electronics in 1982. Netbackup was first used to protect data on SGI IRIX systems. Dell Networker, part of the Dell Technologies Data Protection Suite, was originally developed by Legato Systems, Inc and released in 1990. EMC Corporation acquired Legato in 2002, being subsequently acquired itself by Dell in 2016. Arcserve was originally developed by Cheyenne in 1990. Single & Multi-Machine We can divide the history of commercial data protection solutions into two parts. There are software products which are designed as a single-machine solution and those designed for multi-machine environments. Single-machine solutions typically run on one server and protect the data for that machine, using local media (or increasingly, the public cloud). These products co-exist with the application or applications running on the server. Multi-machine solutions offer more enterprise-class capabilities and protect many individual physical or virtual machines. They generally run on dedicated infrastructure, either one or more physical servers or virtual instances. Multi-machine solutions are designed to support large-scale computing environments where a key requirement is to scale out the data protection process for both backup and restore. Backup Appliances The appliance model has been a popular choice for smaller organisations looking to implement data protection without needing significant investment in skills and resources. There have been two forms of backup appliances over the last two decades. The initial set of solutions acted purely as secondary data stores, optimised for the long-term retention of backup data. This category includes, for example, deduplicating appliances and virtual tape libraries (VTLs). The second option incorporates data protection software into the appliance itself. Rather than deploy software onto a hardware or virtual instance configuration, the appliance comes pre-installed with vendor backup software designed to work with the appliance capabilities. Typically, the appliance configuration is managed by the vendor and updated automatically or through a scheduled approach. The customer benefits from a backup appliance as there are few skills (or time and effort) required to manage the infrastructure. In geographically dispersed businesses, this has particular benefit by removing the requirement for on-site trained staff. Most appliances can be managed remotely. SaaS Over the last decade, data protection has evolved from the bespoke software and appliance models to software-as-a-service. In this option, data protection is delivered either from managed service provider infrastructure or as an application running in the public cloud. The customer does not need to install any on-site equipment, although network bandwidth in and out of the customer\u2019s data centre becomes a factor. SaaS offerings can protect on-premises infrastructure, edge locations, the public cloud and other SaaS services. The multi-faceted nature of SaaS is a boon for businesses that may have little or no on-premises infrastructure and don\u2019t want to be required to deploy hardware to take independent backups. As businesses increasingly rely on SaaS and managed services such as public cloud, we believe that SaaS data protection will be a major force in the industry and become the dominant deployment model. We can envisage this happening due to several factors. Infinite scale. SaaS offers customers effectively infinite scaling capability. The more well-designed SaaS platforms scale backup and restore on demand, using the features of public clouds as a mechanism to optimise cost and scale up to meet unpredictable demand. This removes the requirement for the customer to plan data protection capacity management, as would be required on-premises. Continuous development. In an on-premises model, data protection software is generally managed by the customer (with perhaps the exception of the appliance model). In SaaS offerings, the vendor can update and enhance the service with new features and functionality, on an almost continuous basis. There are limitations, of course. No customer wants to see their familiar interface changing frequently. However, we have all become comfortable with the concept of regular app updates on our mobile devices, so the leap for the enterprise is not difficult to see. Responsiveness. One major aspect of modern data protection is the need to detect and mitigate hacking, malware attacks and ransomware. The profile of attacks is constantly changing, which requires detection and mitigation software to also change at a rapid pace. Delivering data protection as a service enables vendors to be responsive in evolving threat detection technology and making it available to the customer in a timely fashion. Additionally, the vendor has immediate access to the shared knowledge of potentially thousands of customers at the same time. This provides the capability to offer more dynamic protection if a widespread malware attack occurs. Air Gap Security. One method of attack used by ransomware groups is to target both primary and backup storage systems. With SaaS data protection, the secondary copy of data is usually (but not always) stored within the security domain of the vendor. This enables additional security features, such as time locking of snapshots or backups, to be implemented in a way that cannot be overridden, even if the customer\u2019s primary security domain is breached. Forensic Analysis. With secondary data stored in the public cloud, SaaS vendors can more easily implement data analysis and forensic services such as cleanrooms. In modern data recovery, particularly from ransomware, a critical requirement is the ability to test and validate for known good backups. This process is much easier when your data protection vendor is already in the public cloud and can automate the entire process. Although SaaS seems like the point of convergence for all data protection, there are some caveats to consider. Secondary data stored in a SaaS platform is likely to be accessible only for the lifetime of the service subscription. If the service is terminated (or the customer ceases paying), then access to that content may be curtailed. Of course, there is an option here for SaaS data protection vendors to offer a reduced cost \u201cread-only\u201d service, if the customer moves to another provider. As with on-premises data protection software, the format in which the secondary data is stored is generally proprietary. This is also the case in SaaS platforms. However, whereas on-premises an IT team could choose to retain physical media and do bespoke restores where necessary, this facility doesn\u2019t exist for SaaS solutions. One final point to consider is the impact of data sovereignty and legal discovery. When a business owns its own data, it manages security, encryption and the physical location of that data, with the option to put that data out of reach of law enforcement agencies, for example. In SaaS platforms, that capability may not be so simple. Businesses choosing SaaS data protection therefore need to think carefully where their data will be stored and processed, with respect to country-specific data protection rules. The Evolution of Data Stores In the ETIL model, primary data is extracted, transformed, indexed and then loaded into a secondary storage solution. Historically, this process has evolved from the use of tape media to disk systems and now the public cloud as a target repository. Irrespective of the storage medium (and there is no reason why several may not be used in parallel), the destination for secondary data is generally referred to as a \u201cstorage target\u201d. Tape was the first widespread target medium for secondary data. It was relatively cheap (compared to disk systems), with easy portability. This made it possible to transport tape reels to geographically diverse locations for additional data safety. It also makes it possible to optimise the use of tape drives, when data is inactive 90-99% of the time. Tape evolved quickly during the 1980s and 1990s, with IBM leading the way in the enterprise. The tape cartridge format was first introduced in 1984, quickly replacing reel-to-reel tape as the standard form factor. Cartridges are secure, portable and easy to transport and store. However, as with any mass media, businesses quickly encountered \u201ctape sprawl\u201d, with a significant volume of manual media handling required. Technology firms saw an opportunity to automate, with the introduction of automated tape libraries from the 1990s onwards. These systems employed robotic arms to retrieve tapes from a storage slot and insert them into a drive mechanism in an entirely automated process. Despite high degrees of automation, tape still has one Achilles Heel \u2013 it is a sequential read and write medium. If data is stored towards the end of a tape, the entire contents must be spooled through to reach the desired read point. Access time that could be milliseconds or microseconds on hard drives or SSDs can be minutes on tape. In addition, it is extremely difficult to write-in-place to a tape cartridge, unless the data being overwritten is the same length or shorter than the available space. As tapes are written and overwritten, content becomes out of sequence and fragmented. In the 1980s and 1990s, the solution employed by platforms such as DFHSM and Netbackup was to restack or re-order data, reading data from partially filled tapes, consolidating it and creating fully utilised ones. While this process works, it ties up at least two tape drives, plus the media being consolidated. As tape media increased in capacity, the time required for consolidation becomes excessive for tapes that are constantly in use. The answer to the problem emerged in two disk-based technologies \u2013 virtual tape libraries and deduplicating appliances. Virtual Tape Tape drives are relatively simple devices, conforming to the SCSI protocol, whether directly connected, or attached via SAS or Fibre Channel transports. These commands can be emulated through software, creating the concept of a virtual tape device that appears to operate like a physical tape library. As disk systems became much cheaper in the early 2000s, so virtual tape libraries offered an alternative for frequently accessed data stored on tape. Statistically, backups are most frequently used within the first few hours and days from creation, after which their use declines over time. VTLs provide a mechanism to store and retrieve backups during the initial period of activity, eventually archiving backups to tape for longer-term retention. VTLs provide much faster \u201ctime to first byte\u201d than tape systems, which must spool a tape to the initial reading position before starting to read data. They also reduce the impact of fragmentation and restacking, as short-lived backups can be retained purely on disk. De-duplication While VTLs offer mitigation for the sequential nature of tape, they still store significant volumes of essentially duplicated data. As an example, many backup policies traditionally took full copies of data on a weekly basis, keeping that data for up to six months. Between full backups, incremental changes are identified and kept separately. When hundreds of physical servers (or thousands of virtual servers) are based on a standardised image, the degree of duplication in secondary backup data can be substantial. De-duplicating appliances emerged in the early 2000s, as a way to reduce the volume of secondary data being stored on disk systems. As data is ingested, the content is split into chunks and fingerprinted (a process called hashing). Data with new hashes is stored and indexed against the source, while data that shows a known hash is also indexed but the physical data is discarded. Deduplication relies on hashing techniques that produce unique values from a specific piece of content, which historically took significant computing power to achieve. Some primary storage systems today use weak hashing algorithms and manage hash \u201ccollisions\u201d with an exception process as a way to reduce CPU usage. While deduplication appliances are a powerful way to reduce secondary data storage volumes, they do have some challenges. Firstly, on early disk-based systems, over time the hashing process essentially creates an entirely random data set on disk, which results in performance problems when reading data during a restore. Vendors have mitigated these issues with content-aware placement and flash-based systems. Second, the ability to access and restore data relies entirely on the accuracy of the hash indexing held by the deduplication appliance. If that data is lost or corrupted, then accessing the data becomes impossible. This creates a significant single point of failure in design that vendors must explain how they mitigate.","title":"5. A Short History of Data Protection"},{"location":"chapter5/#a-short-history-of-data-protection","text":"Data protection has been a key feature of computer systems since the first commercial computers entered production in 1951. For at least two decades, backup and recovery of data was a bespoke solution implemented by systems programmers, rather than a product which could be bought and installed on the system. Part of the challenge for system operators at that time was the lack of suitable offline media. UNISERVO, a tape system based on heavy steel tape, was introduced for the UNIVAC I in 1951. IBM introduced its first tape solution, the IBM 726, with the IBM 701 in 1952. Reel-to-Reel tape persisted as the standard medium, based on magnetic coated film, until the mid-1980s, with the introduction of the IBM 3480 tape cartridge. On the IBM mainframe, Hierarchical Storage Manager was introduced by IBM on the MVS operating system and System/370 platform in 1977. Version 2 was renamed DFHSM (Data Facility Hierarchical Storage Manager) and eventually DFSMShsm with the introduction of system-managed storage and the Storage Management Subsystem (DFSMS) in the late 1980s. Elsewhere, Unix systems generally relied on native commands, including tar, introduced in Unix Version 7 in 1979. The tar utility was preceded by tp and tap commands. Other utilities such as cp and cpio offered the capability to transfer data to and from tape. Many of the data protection solutions we know today had origins in bespoke or custom software tools. BackupExec derived from tape streaming software developed by Maynard Electronics in 1982. Netbackup was first used to protect data on SGI IRIX systems. Dell Networker, part of the Dell Technologies Data Protection Suite, was originally developed by Legato Systems, Inc and released in 1990. EMC Corporation acquired Legato in 2002, being subsequently acquired itself by Dell in 2016. Arcserve was originally developed by Cheyenne in 1990.","title":"A Short History of Data Protection"},{"location":"chapter5/#single-multi-machine","text":"We can divide the history of commercial data protection solutions into two parts. There are software products which are designed as a single-machine solution and those designed for multi-machine environments. Single-machine solutions typically run on one server and protect the data for that machine, using local media (or increasingly, the public cloud). These products co-exist with the application or applications running on the server. Multi-machine solutions offer more enterprise-class capabilities and protect many individual physical or virtual machines. They generally run on dedicated infrastructure, either one or more physical servers or virtual instances. Multi-machine solutions are designed to support large-scale computing environments where a key requirement is to scale out the data protection process for both backup and restore.","title":"Single &amp; Multi-Machine"},{"location":"chapter5/#backup-appliances","text":"The appliance model has been a popular choice for smaller organisations looking to implement data protection without needing significant investment in skills and resources. There have been two forms of backup appliances over the last two decades. The initial set of solutions acted purely as secondary data stores, optimised for the long-term retention of backup data. This category includes, for example, deduplicating appliances and virtual tape libraries (VTLs). The second option incorporates data protection software into the appliance itself. Rather than deploy software onto a hardware or virtual instance configuration, the appliance comes pre-installed with vendor backup software designed to work with the appliance capabilities. Typically, the appliance configuration is managed by the vendor and updated automatically or through a scheduled approach. The customer benefits from a backup appliance as there are few skills (or time and effort) required to manage the infrastructure. In geographically dispersed businesses, this has particular benefit by removing the requirement for on-site trained staff. Most appliances can be managed remotely.","title":"Backup Appliances"},{"location":"chapter5/#saas","text":"Over the last decade, data protection has evolved from the bespoke software and appliance models to software-as-a-service. In this option, data protection is delivered either from managed service provider infrastructure or as an application running in the public cloud. The customer does not need to install any on-site equipment, although network bandwidth in and out of the customer\u2019s data centre becomes a factor. SaaS offerings can protect on-premises infrastructure, edge locations, the public cloud and other SaaS services. The multi-faceted nature of SaaS is a boon for businesses that may have little or no on-premises infrastructure and don\u2019t want to be required to deploy hardware to take independent backups. As businesses increasingly rely on SaaS and managed services such as public cloud, we believe that SaaS data protection will be a major force in the industry and become the dominant deployment model. We can envisage this happening due to several factors. Infinite scale. SaaS offers customers effectively infinite scaling capability. The more well-designed SaaS platforms scale backup and restore on demand, using the features of public clouds as a mechanism to optimise cost and scale up to meet unpredictable demand. This removes the requirement for the customer to plan data protection capacity management, as would be required on-premises. Continuous development. In an on-premises model, data protection software is generally managed by the customer (with perhaps the exception of the appliance model). In SaaS offerings, the vendor can update and enhance the service with new features and functionality, on an almost continuous basis. There are limitations, of course. No customer wants to see their familiar interface changing frequently. However, we have all become comfortable with the concept of regular app updates on our mobile devices, so the leap for the enterprise is not difficult to see. Responsiveness. One major aspect of modern data protection is the need to detect and mitigate hacking, malware attacks and ransomware. The profile of attacks is constantly changing, which requires detection and mitigation software to also change at a rapid pace. Delivering data protection as a service enables vendors to be responsive in evolving threat detection technology and making it available to the customer in a timely fashion. Additionally, the vendor has immediate access to the shared knowledge of potentially thousands of customers at the same time. This provides the capability to offer more dynamic protection if a widespread malware attack occurs. Air Gap Security. One method of attack used by ransomware groups is to target both primary and backup storage systems. With SaaS data protection, the secondary copy of data is usually (but not always) stored within the security domain of the vendor. This enables additional security features, such as time locking of snapshots or backups, to be implemented in a way that cannot be overridden, even if the customer\u2019s primary security domain is breached. Forensic Analysis. With secondary data stored in the public cloud, SaaS vendors can more easily implement data analysis and forensic services such as cleanrooms. In modern data recovery, particularly from ransomware, a critical requirement is the ability to test and validate for known good backups. This process is much easier when your data protection vendor is already in the public cloud and can automate the entire process. Although SaaS seems like the point of convergence for all data protection, there are some caveats to consider. Secondary data stored in a SaaS platform is likely to be accessible only for the lifetime of the service subscription. If the service is terminated (or the customer ceases paying), then access to that content may be curtailed. Of course, there is an option here for SaaS data protection vendors to offer a reduced cost \u201cread-only\u201d service, if the customer moves to another provider. As with on-premises data protection software, the format in which the secondary data is stored is generally proprietary. This is also the case in SaaS platforms. However, whereas on-premises an IT team could choose to retain physical media and do bespoke restores where necessary, this facility doesn\u2019t exist for SaaS solutions. One final point to consider is the impact of data sovereignty and legal discovery. When a business owns its own data, it manages security, encryption and the physical location of that data, with the option to put that data out of reach of law enforcement agencies, for example. In SaaS platforms, that capability may not be so simple. Businesses choosing SaaS data protection therefore need to think carefully where their data will be stored and processed, with respect to country-specific data protection rules. The Evolution of Data Stores In the ETIL model, primary data is extracted, transformed, indexed and then loaded into a secondary storage solution. Historically, this process has evolved from the use of tape media to disk systems and now the public cloud as a target repository. Irrespective of the storage medium (and there is no reason why several may not be used in parallel), the destination for secondary data is generally referred to as a \u201cstorage target\u201d. Tape was the first widespread target medium for secondary data. It was relatively cheap (compared to disk systems), with easy portability. This made it possible to transport tape reels to geographically diverse locations for additional data safety. It also makes it possible to optimise the use of tape drives, when data is inactive 90-99% of the time. Tape evolved quickly during the 1980s and 1990s, with IBM leading the way in the enterprise. The tape cartridge format was first introduced in 1984, quickly replacing reel-to-reel tape as the standard form factor. Cartridges are secure, portable and easy to transport and store. However, as with any mass media, businesses quickly encountered \u201ctape sprawl\u201d, with a significant volume of manual media handling required. Technology firms saw an opportunity to automate, with the introduction of automated tape libraries from the 1990s onwards. These systems employed robotic arms to retrieve tapes from a storage slot and insert them into a drive mechanism in an entirely automated process. Despite high degrees of automation, tape still has one Achilles Heel \u2013 it is a sequential read and write medium. If data is stored towards the end of a tape, the entire contents must be spooled through to reach the desired read point. Access time that could be milliseconds or microseconds on hard drives or SSDs can be minutes on tape. In addition, it is extremely difficult to write-in-place to a tape cartridge, unless the data being overwritten is the same length or shorter than the available space. As tapes are written and overwritten, content becomes out of sequence and fragmented. In the 1980s and 1990s, the solution employed by platforms such as DFHSM and Netbackup was to restack or re-order data, reading data from partially filled tapes, consolidating it and creating fully utilised ones. While this process works, it ties up at least two tape drives, plus the media being consolidated. As tape media increased in capacity, the time required for consolidation becomes excessive for tapes that are constantly in use. The answer to the problem emerged in two disk-based technologies \u2013 virtual tape libraries and deduplicating appliances.","title":"SaaS"},{"location":"chapter5/#virtual-tape","text":"Tape drives are relatively simple devices, conforming to the SCSI protocol, whether directly connected, or attached via SAS or Fibre Channel transports. These commands can be emulated through software, creating the concept of a virtual tape device that appears to operate like a physical tape library. As disk systems became much cheaper in the early 2000s, so virtual tape libraries offered an alternative for frequently accessed data stored on tape. Statistically, backups are most frequently used within the first few hours and days from creation, after which their use declines over time. VTLs provide a mechanism to store and retrieve backups during the initial period of activity, eventually archiving backups to tape for longer-term retention. VTLs provide much faster \u201ctime to first byte\u201d than tape systems, which must spool a tape to the initial reading position before starting to read data. They also reduce the impact of fragmentation and restacking, as short-lived backups can be retained purely on disk.","title":"Virtual Tape"},{"location":"chapter5/#de-duplication","text":"While VTLs offer mitigation for the sequential nature of tape, they still store significant volumes of essentially duplicated data. As an example, many backup policies traditionally took full copies of data on a weekly basis, keeping that data for up to six months. Between full backups, incremental changes are identified and kept separately. When hundreds of physical servers (or thousands of virtual servers) are based on a standardised image, the degree of duplication in secondary backup data can be substantial. De-duplicating appliances emerged in the early 2000s, as a way to reduce the volume of secondary data being stored on disk systems. As data is ingested, the content is split into chunks and fingerprinted (a process called hashing). Data with new hashes is stored and indexed against the source, while data that shows a known hash is also indexed but the physical data is discarded. Deduplication relies on hashing techniques that produce unique values from a specific piece of content, which historically took significant computing power to achieve. Some primary storage systems today use weak hashing algorithms and manage hash \u201ccollisions\u201d with an exception process as a way to reduce CPU usage. While deduplication appliances are a powerful way to reduce secondary data storage volumes, they do have some challenges. Firstly, on early disk-based systems, over time the hashing process essentially creates an entirely random data set on disk, which results in performance problems when reading data during a restore. Vendors have mitigated these issues with content-aware placement and flash-based systems. Second, the ability to access and restore data relies entirely on the accuracy of the hash indexing held by the deduplication appliance. If that data is lost or corrupted, then accessing the data becomes impossible. This creates a significant single point of failure in design that vendors must explain how they mitigate.","title":"De-duplication"},{"location":"chapter6/","text":"Diving Deeper into Data Protection As our ETIL process described earlier shows us the process for data extraction and storage, we will start a deeper dive of the data protection ecosystem by looking at storage media. Typically, the data in backup systems is referred to as \"secondary\" data (compared to \"primary\" data) as it isn't directly accessed by production applications, other than backup software. Secondary data can be stored on tape, disk, NAND flash or on the public cloud, which is an ofuscated version of the first set of technologies. Media Requirements The characteristics of secondary data are generally different from that of primary data in computing systems. Primary data resides on systems that deliver high random access I/O performance and good throughput. Random access refers to the capability to retrieve any individual piece of data directly, compared to sequential access that requires traversing media until the required data is reached. Tape, for example, is a sequential medium, as the tape must be spooled or read from the start to the required access point. In contrast, HDD and SSD are random media that can be accessed by logical byte or block address. Secondary data access used to be solely focused on throughput speed. Backup time is measured by the throughput capability of secondary media. The faster data can be written to tape or disk, the quicker the backup will complete. Equally, the recovery time for data is also based on throughput. The faster data can be retrieved from secondary storage, the shorter the recovery process. As we will discuss later, data optimisation techniques have changed that paradigm somewhat, as data deduplication causes changes sequential I/O into almost totally random I/O.","title":"6. Diving Deeper into Data Protection"},{"location":"chapter6/#diving-deeper-into-data-protection","text":"As our ETIL process described earlier shows us the process for data extraction and storage, we will start a deeper dive of the data protection ecosystem by looking at storage media. Typically, the data in backup systems is referred to as \"secondary\" data (compared to \"primary\" data) as it isn't directly accessed by production applications, other than backup software. Secondary data can be stored on tape, disk, NAND flash or on the public cloud, which is an ofuscated version of the first set of technologies.","title":"Diving Deeper into Data Protection"},{"location":"chapter6/#media-requirements","text":"The characteristics of secondary data are generally different from that of primary data in computing systems. Primary data resides on systems that deliver high random access I/O performance and good throughput. Random access refers to the capability to retrieve any individual piece of data directly, compared to sequential access that requires traversing media until the required data is reached. Tape, for example, is a sequential medium, as the tape must be spooled or read from the start to the required access point. In contrast, HDD and SSD are random media that can be accessed by logical byte or block address. Secondary data access used to be solely focused on throughput speed. Backup time is measured by the throughput capability of secondary media. The faster data can be written to tape or disk, the quicker the backup will complete. Equally, the recovery time for data is also based on throughput. The faster data can be retrieved from secondary storage, the shorter the recovery process. As we will discuss later, data optimisation techniques have changed that paradigm somewhat, as data deduplication causes changes sequential I/O into almost totally random I/O.","title":"Media Requirements"}]}